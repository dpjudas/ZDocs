<page xmlns="http://www.mediawiki.org/xml/export-0.11/">
  <title>Downloading the ZDoom Wiki</title>
  <ns>0</ns>
  <id>2966</id>
  <revision>
    <id>38798</id>
    <parentid>38710</parentid>
    <timestamp>2015-02-06T21:04:33Z</timestamp>
    <contributor>
      <username>Xeotroid</username>
      <id>2024</id>
    </contributor>
    <minor />
    <comment>Please don't ban me for downloading &lt;x&gt; history pages because they weren't in the tutorial. :D</comment>
    <origin>38798</origin>
    <model>wikitext</model>
    <format>text/x-wiki</format>
    <text bytes="1874" sha1="k9iavbaz301ih3hge76b3wk3xc1i9io" xml:space="preserve">Instructions for downloading the ZDoom Wiki so that is viewable off-line:

# Download HTTrack from the HTTrack Homepage: http://www.httrack.com
# Configure the filter under the "Scan Rules" tab in the Set Options menu to include this: 
:* &lt;code&gt;-*Special:*&lt;/code&gt;
:* &lt;code&gt;-*Talk:*&lt;/code&gt;
:* &lt;code&gt;-*User:*&lt;/code&gt;
:* &lt;code&gt;-*&amp;action=*&lt;/code&gt;
:* &lt;code&gt;-*&amp;printable=*&lt;/code&gt;
:* &lt;code&gt;-*&amp;oldid=*&lt;/code&gt;
# In the "Spider" tab, make sure the drop-down field containing "robots.txt" is set to "no robots.txt rules", and make sure that the “Force old HTTP/1.0 requests (no 1.1)” box is checked.
# Point HTTrack to the following web address: http://www.zdoom.org/wiki
# Run HTTrack.

&lt;sup id="ref_Grubber"&gt;[[Downloading the ZDoom Wiki#endnote_Grubber|[1]]]&lt;/sup&gt;

The program will rip all the pages from the site and store them in a directory on your hard drive.  Download times will vary, depending on your connection, anywhere between two minutes to an hour.

After that, you can browse the ZDoom Wiki off-line just like you would online.

If you have wget installed, you can also use the following command to download the wiki:

 &lt;nowiki&gt;wget -rk http://www.zdoom.org/wiki/Main_Page&lt;/nowiki&gt;

==Warning==
Failure to properly configure your web leaching program is liable to get you blocked from the site. If I see that there is unusually high network traffic for hours, and when I check my logs, I see that somebody is downloading '''''every single page''''' on the wiki, including discussion pages, edit pages, history pages, and every special page possible, I will block them at my own discretion. - [[User:Randy Heit|Randy Heit]] ([[User talk:Randy Heit|talk]]) 19:34, 18 January 2015 (CST)

==References==

#&lt;cite id="endnote_Grubber" style="font-style: normal;"&gt;[[#ref_Grubber|'''↑''']]&lt;/cite&gt;Configuration and settings obtained from Grubber.

[[Category:Wiki guides]]</text>
    <sha1>k9iavbaz301ih3hge76b3wk3xc1i9io</sha1>
  </revision>
</page>